{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "--2023-12-04 12:53:34--  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 638660563 (609M) [application/x-gzip]\n",
      "Saving to: ‘hadoop-3.3.2.tar.gz.1’\n",
      "\n",
      "hadoop-3.3.2.tar.gz 100%[===================>] 609.07M  15.2MB/s    in 51s     \n",
      "\n",
      "2023-12-04 12:54:26 (11.9 MB/s) - ‘hadoop-3.3.2.tar.gz.1’ saved [638660563/638660563]\n",
      "\n",
      "/bin/bash: hadoop: command not found\n",
      "/bin/bash: hadoop: command not found\n",
      "/bin/bash: hadoop: command not found\n",
      "22503 Jps\n",
      "14109 SparkSubmit\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
    "!tar xf hadoop-3.3.2.tar.gz\n",
    "\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/content/hadoop-3.3.2/etc/hadoop\"\n",
    "!hadoop namenode -format\n",
    "!hadoop --daemon start namenode\n",
    "!hadoop --daemon start datanode\n",
    "!jps\n",
    "os.environ[\"PATH\"] += os.pathsep + \"$HADOOP_HOME/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=a611a7e112f94ee1f5cfdcd5129a446c87e227ef9aaec238ed7694330aebbb6d\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 13:30:59 WARN Utils: Your hostname, codespaces-aff66d resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/12/04 13:30:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/04 13:31:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namenode hostname: /\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "namenode_hostname = spark._jsc.hadoopConfiguration().get(\"fs.defaultFS\").split(\"//\")[1].split(\":\")[0]\n",
    "\n",
    "print(\"Namenode hostname:\", namenode_hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from python-docx) (4.8.0)\n",
      "Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: lxml, python-docx\n",
      "Successfully installed lxml-4.9.3 python-docx-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cubilia 1\n",
      "curae; 1\n",
      "Pellentesque 1\n",
      "lacinia 1\n",
      "dictum 3\n",
      "pharetra. 1\n",
      "Vestibulum 2\n",
      "lectus 1\n",
      "magna 2\n",
      "fermentum 3\n",
      "felis 2\n",
      "ornare, 1\n",
      "quis 7\n",
      "ex 4\n",
      "Duis 4\n",
      "porttitor 3\n",
      "suscipit. 1\n",
      "Aliquam 1\n",
      "bibendum 1\n",
      "leo, 1\n",
      "condimentum 4\n",
      "ante 3\n",
      "scelerisque 2\n",
      "quis. 1\n",
      "Interdum 1\n",
      "malesuada 4\n",
      "ipsum 5\n",
      "primis 2\n",
      "in 4\n",
      "dui. 1\n",
      "amet 7\n",
      "Nunc 2\n",
      "eget 6\n",
      "mi 2\n",
      "ac, 3\n",
      "feugiat 3\n",
      "augue 5\n",
      "at 3\n",
      "Cras 5\n",
      "rhoncus 1\n",
      "ac. 2\n",
      "varius, 1\n",
      "ultricies, 1\n",
      "metus 2\n",
      "mattis 1\n",
      "enim. 2\n",
      "rutrum 4\n",
      "sodales 2\n",
      "fringilla. 1\n",
      "condimentum. 1\n",
      "Quisque 1\n",
      "Sed 3\n",
      "sed 5\n",
      "sapien 4\n",
      "faucibus 2\n",
      "ornare 3\n",
      "lorem, 1\n",
      "vitae 7\n",
      "varius 3\n",
      "Vivamus 1\n",
      "sapien. 1\n",
      "Aenean 4\n",
      "est 2\n",
      "mollis 1\n",
      "sollicitudin 3\n",
      "nisi 2\n",
      "tincidunt. 2\n",
      "Proin 1\n",
      "interdum 1\n",
      "amet. 2\n",
      "maximus 2\n",
      "massa. 3\n",
      "Maecenas 2\n",
      "elit. 2\n",
      "Lorem 1\n",
      "consectetur 1\n",
      "diam 1\n",
      "porta, 1\n",
      "facilisis. 1\n",
      "commodo 2\n",
      "sagittis 1\n",
      "et. 2\n",
      "lacinia. 1\n",
      "nibh 3\n",
      "finibus 1\n",
      "felis. 1\n",
      "rutrum, 1\n",
      "quam 2\n",
      "molestie 1\n",
      "augue, 1\n",
      "augue. 1\n",
      "condimentum, 1\n",
      "ut, 1\n",
      "justo. 1\n",
      "cursus 1\n",
      "lacus 2\n",
      "lorem. 1\n",
      "pellentesque 2\n",
      "dolor, 1\n",
      "purus 1\n",
      "vulputate, 1\n",
      "Nam 3\n",
      "nulla 2\n",
      "feugiat. 2\n",
      "ullamcorper 1\n",
      "posuere. 1\n",
      "venenatis 1\n",
      "lobortis 1\n",
      "lectus, 1\n",
      "eget. 1\n",
      "Orci 1\n",
      "natoque 1\n",
      "dis 1\n",
      "nascetur 1\n",
      "mus. 1\n",
      "diam, 1\n",
      "urna 1\n",
      "odio 1\n",
      "nunc 1\n",
      "posuere 7\n",
      "enim 7\n",
      "mauris, 2\n",
      "id 7\n",
      "gravida 1\n",
      "et 10\n",
      "vel 4\n",
      "accumsan 3\n",
      "efficitur. 1\n",
      "Nullam 3\n",
      "tortor 2\n",
      "nec 6\n",
      "porttitor. 2\n",
      "consequat 3\n",
      "fames 1\n",
      "ac 4\n",
      "faucibus. 1\n",
      "a 5\n",
      "luctus 4\n",
      "Phasellus 2\n",
      "dui 3\n",
      "arcu, 2\n",
      "sit 10\n",
      "laoreet 6\n",
      "a, 1\n",
      "pulvinar 3\n",
      "risus. 1\n",
      "neque 2\n",
      "ullamcorper, 1\n",
      "aliquam 2\n",
      "velit. 1\n",
      "eleifend 3\n",
      "eu 7\n",
      "leo. 1\n",
      "In 3\n",
      "tincidunt 4\n",
      "consectetur, 1\n",
      "blandit. 1\n",
      "ante, 1\n",
      "imperdiet 5\n",
      "Suspendisse 3\n",
      "arcu 3\n",
      "ligula 5\n",
      "semper 3\n",
      "justo, 1\n",
      "auctor 1\n",
      "dolor 6\n",
      "Etiam 1\n",
      "ultricies 1\n",
      "blandit 2\n",
      "non 1\n",
      "efficitur 2\n",
      "egestas. 1\n",
      "eros 6\n",
      "dapibus, 1\n",
      "hendrerit 1\n",
      "diam. 1\n",
      "eros. 1\n",
      "Nulla 1\n",
      "porta 1\n",
      "libero. 1\n",
      "libero 2\n",
      "vitae. 1\n",
      "vestibulum 3\n",
      "placerat 3\n",
      "porta. 1\n",
      "tempor 4\n",
      "nulla. 1\n",
      "Integer 5\n",
      "facilisis 2\n",
      "Praesent 1\n",
      "ut 2\n",
      "odio. 1\n",
      "Ut 2\n",
      "dapibus. 1\n",
      "lacus, 1\n",
      "viverra 2\n",
      "quam, 1\n",
      "purus, 1\n",
      "dignissim 2\n",
      "amet, 1\n",
      "adipiscing 1\n",
      "Mauris 1\n",
      "elit 1\n",
      "felis, 1\n",
      "ultrices 2\n",
      "nibh, 1\n",
      "vitae, 1\n",
      "pretium 1\n",
      "Curabitur 3\n",
      "risus 2\n",
      "maximus, 1\n",
      "tristique 1\n",
      "turpis 1\n",
      "lorem 1\n",
      "tincidunt, 1\n",
      "est. 1\n",
      "lectus. 1\n",
      "fermentum. 1\n",
      "nunc, 1\n",
      "id. 1\n",
      "aliquet 2\n",
      "sem 2\n",
      "ultrices. 1\n",
      "metus. 1\n",
      "Morbi 1\n",
      "arcu. 1\n",
      "laoreet. 1\n",
      "magna. 1\n",
      "volutpat 1\n",
      "penatibus 1\n",
      "magnis 1\n",
      "parturient 1\n",
      "montes, 1\n",
      "ridiculus 1\n",
      "Fusce 1\n",
      "aliquet, 1\n",
      "gravida, 1\n",
      "massa 1\n",
      "odio, 1\n",
      "orci 2\n",
      "Total word count: 496\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import docx\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read Word document using python-docx\n",
    "file_path = \"test.docx\"\n",
    "doc = docx.Document(file_path)\n",
    "\n",
    "# Extract text from the Word document\n",
    "paragraphs = doc.paragraphs\n",
    "words = []\n",
    "for paragraph in paragraphs:\n",
    "    words += paragraph.text.split()\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Convert words to RDD\n",
    "words_rdd = spark.sparkContext.parallelize(words)\n",
    "\n",
    "# Save words to local file if it does not exist\n",
    "local_file_path = \"/tmp/wordscount.txt\"\n",
    "if not os.path.exists(local_file_path):\n",
    "    words_rdd.saveAsTextFile(local_file_path)\n",
    "\n",
    "# Read words from local file\n",
    "local_words = spark.sparkContext.textFile(local_file_path)\n",
    "\n",
    "# Perform word counting\n",
    "word_counts = local_words.flatMap(lambda line: line.split(\" \"))\\\n",
    "                         .filter(lambda word: isinstance(word, str))\\\n",
    "                         .map(lambda word: (word, 1))\\\n",
    "                         .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Show the total word count\n",
    "total_word_count = word_counts.collect()\n",
    "total = 0\n",
    "for (word, count) in total_word_count:\n",
    "    total+=count\n",
    "    print(word, count)\n",
    "print(\"Total word count:\", total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
